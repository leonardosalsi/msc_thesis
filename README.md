
# Msc Thesis - The Impact of Sequence Diversity on DNA Foundation Model Performance

## Models and Datasets

Datasets are downloaded using the [Hugging Face `datasets` library](https://huggingface.co/docs/datasets/). Models and tokenizers are downloaded using
the [Hugging Face `transformers` library](https://huggingface.co/docs/transformers/).
When the relevant code is executed, the library will automatically fetch and store the required datasets in this directory.

It is important to create a file called `config.py` in the root of the project which defines the following variables

```python
models_cache_dir = "<PATH_TO_MODELS>"
datasets_cache_dir = "<PATH_TO_DATASETS>"
LOGLEVEL=22
```
When `preload_models.py` and `preload_datasets.py` are executed for the first time, all necessary data is downloaded
to the paths specified in `config.py`. Splits are generated and stored for future use in the folder specified by
`datasets_cache_dir`.

### Used Models

1. **[InstaDeepAI/nucleotide-transformer-v2-50m-multi-species](https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species)**
2. **[InstaDeepAI/nucleotide-transformer-v2-100m-multi-species](https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-100m-multi-species)**
3. **[InstaDeepAI/nucleotide-transformer-v2-250m-multi-species](https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-250m-multi-species)**
4. **[InstaDeepAI/nucleotide-transformer-v2-500m-multi-species](https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species)**

### Used Datasets

1. **[InstaDeepAI/multi_species_genomes](https://huggingface.co/datasets/InstaDeepAI/multi_species_genomes)**
2. **[InstaDeepAI/nucleotide_transformer_downstream_tasks_revised](https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks_revised)**
3. **[katarinagresova/Genomic_Benchmarks_human_ensembl_regulatory](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_human_ensembl_regulatory)**
4. **[katarinagresova/Genomic_Benchmarks_demo_human_or_worm](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_demo_human_or_worm)**
5. **[katarinagresova/Genomic_Benchmarks_human_ocr_ensembl](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_human_ocr_ensembl)**
6. **[katarinagresova/Genomic_Benchmarks_drosophila_enhancers_stark](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_drosophila_enhancers_stark)**
7. **[katarinagresova/Genomic_Benchmarks_dummy_mouse_enhancers_ensembl](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_dummy_mouse_enhancers_ensembl)**
8. **[katarinagresova/Genomic_Benchmarks_demo_coding_vs_intergenomic_seqs](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_demo_coding_vs_intergenomic_seqs)**
9. **[katarinagresova/Genomic_Benchmarks_human_enhancers_ensembl](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_human_enhancers_ensembl)**
10. **[katarinagresova/Genomic_Benchmarks_human_enhancers_cohn](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_human_enhancers_cohn)**
11. **[katarinagresova/Genomic_Benchmarks_human_nontata_promoters](https://huggingface.co/datasets/katarinagresova/Genomic_Benchmarks_human_nontata_promoters)**

## Evaluation

This repository contains a set of [SLURM](https://slurm.schedmd.com/documentation.html) jobs that have been 
autogenerated by `downstream_tasks.py` which are stored in `jobs/evaluate_mcc`. Each job is designed to train one
model on one downstream task and evaluate it by its MCC.

The training parameters are equal for all combinations of models and downstream tasks and have been borrowed from
an example provided by [Hugging Face](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb).

Each job calls `evaluate_model_mcc.py` and selects the model and the downstream task via available parameter.

> **NOTE** One is able to run `evaluate_model_mcc.py` directly in their machine. All models and downstream tasks
> have unique identifier, which are used when training and evaluating a model. Furthermore, one can decide to 
> use models with random weights instead of pretrained models and whether to employ LoRA or not.

```shell
evaluate_model_mcc.py modelId taskId [-h] [--random-weights] [--no-lora]
```

> **NOTE** Currently, `evaluate_model_mcc.py` expects all models and datasets to be downlaoded already. A WIP is the
> possibility to pass the variable which decides whether data should be downloaded or looked for in a local directory.

#### TaskIDs

```
1 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/promoter_all
2 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/promoter_tata
3 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/promoter_no_tata
4 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/enhancers
5 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/enhancers_types
6 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/splice_sites_all
7 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/splice_sites_acceptors
8 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/splice_sites_donors
9 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H2AFZ
10 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K27ac
11 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K27me3
12 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K36me3
13 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K4me1
14 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K4me2
15 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K4me3
16 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K9ac
17 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H3K9me3
18 InstaDeepAI/nucleotide_transformer_downstream_tasks_revised/H4K20me1
19 katarinagresova/Genomic_Benchmarks_human_ensembl_regulatory
20 katarinagresova/Genomic_Benchmarks_demo_human_or_worm
21 katarinagresova/Genomic_Benchmarks_human_ocr_ensembl
22 katarinagresova/Genomic_Benchmarks_drosophila_enhancers_stark
23 katarinagresova/Genomic_Benchmarks_dummy_mouse_enhancers_ensembl
24 katarinagresova/Genomic_Benchmarks_demo_coding_vs_intergenomic_seqs
25 katarinagresova/Genomic_Benchmarks_human_enhancers_ensembl
26 katarinagresova/Genomic_Benchmarks_human_enhancers_cohn
27 katarinagresova/Genomic_Benchmarks_human_nontata_promoters
```

#### ModelIDs
```
1 InstaDeepAI/nucleotide-transformer-v2-50m-multi-species
2 InstaDeepAI/nucleotide-transformer-v2-100m-multi-species
3 InstaDeepAI/nucleotide-transformer-v2-250m-multi-species
4 InstaDeepAI/nucleotide-transformer-v2-500m-multi-species
5 InstaDeepAI/nucleotide-transformer-500m-1000g
6 InstaDeepAI/nucleotide-transformer-500m-human-ref
```

### Evaluation Results

In the following are some results from the evaluation. Note that only the variants with 50M and 500M trainable
parameters of NT-Multi-Species V2 have been additionally evaluated in a state of randomized weights alongside their
pretrained variants. All data here depicts a fine-tuning with LoRA. Figures, where LoRA was not employed, can be found
in `img/no-lora`.


![eval_mcc](https://github.com/user-attachments/assets/0eecbf60-6978-42cb-9423-4a50e16e8575)
![norm_mcc_across_tasks](https://github.com/user-attachments/assets/cbecd87e-b079-426a-a34c-4264d40598fd)

### Training Loss
The loss evolution during training was captured. For each downstream task, a figure has been generated showing how each model
evolved over time. This has only been conducted for training with LoRA and the figures can be found in `img/lora/training`.

![H3K4me1](https://github.com/user-attachments/assets/e2774d6f-1cb0-4d23-b8de-14d371b7ae7e)

